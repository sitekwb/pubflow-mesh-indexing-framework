{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import ijson\n",
    "from tqdm import tqdm\n",
    "from indexingcode.utils.Preprocessor import Preprocessor\n",
    "import gensim\n",
    "import nltk\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import List, Dict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_text(node):\n",
    "    try:\n",
    "        return node.text\n",
    "    except AttributeError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 8] nodename\n",
      "[nltk_data]     nor servname provided, or not known>\n"
     ]
    }
   ],
   "source": [
    "preprocessor = Preprocessor(gensim.utils.tokenize, nltk.PorterStemmer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "et = ET.parse('/Volumes/SanDisk/desc2022.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2: 100%|██████████| 30194/30194 [00:00<00:00, 56809.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# load meshes XML\n",
    "text_to_tree = {}\n",
    "root = et.getroot()\n",
    "omit_count = 0\n",
    "with tqdm(root.findall('DescriptorRecord')) as bar:\n",
    "    for record in bar:\n",
    "        tree_text = get_text(record.find('TreeNumberList/TreeNumber'))\n",
    "        name = get_text(record.find('DescriptorName/String'))\n",
    "        if name is None or tree_text is None:\n",
    "            omit_count += 1\n",
    "            bar.set_description(str(omit_count))\n",
    "            continue\n",
    "        # first_tree = tree_text.split('.')[0]\n",
    "        first_tree = tree_text[0]\n",
    "        text_to_tree[name] = first_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "desc_words_set = set()\n",
    "omit_count = 0\n",
    "with tqdm(root.findall('DescriptorRecord')) as bar:\n",
    "    for record in bar:\n",
    "        name = get_text(record.find('DescriptorName/String'))\n",
    "        if name is None:\n",
    "            omit_count += 1\n",
    "            bar.set_description(str(omit_count))\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Num to text dictionary creation: 100%|██████████| 15559157/15559157 [05:58<00:00, 43342.82it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('/Volumes/SanDisk/BioAsq2021/allMeSH_2021.json', encoding='ISO-8859-1') as bioasq_file:\n",
    "    with open('/Volumes/SanDisk/compare_meshes_tree.csv', 'w') as wf:\n",
    "            num_to_text = {}\n",
    "            num_to_tree = {}\n",
    "            tree_to_indexes: Dict[str, List[int]] = {}\n",
    "            omit_count = 0\n",
    "            i = 0\n",
    "            for article in tqdm(ijson.items(bioasq_file, 'articles.item'), total=15559157, desc='Num to text dictionary creation'):\n",
    "                # mesh num to text\n",
    "                nums: List[int] = preprocessor.preprocess_mesh(article['meshMajor'])\n",
    "                for num, text in zip(nums, article['meshMajor']):\n",
    "                    if num not in num_to_text:\n",
    "                        num_to_text[num] = text\n",
    "                        if text in text_to_tree:\n",
    "                            first_tree = text_to_tree[text]\n",
    "                            num_to_tree[num] = first_tree\n",
    "                        else:\n",
    "                            omit_count += 1\n",
    "                trees = list(filter(lambda x: x is not None, [num_to_tree[n] if n in num_to_tree else None for n in nums]))\n",
    "                most_freq_tree = max(set(trees), key=trees.count)\n",
    "                if most_freq_tree not in tree_to_indexes:\n",
    "                    tree_to_indexes[most_freq_tree] = []\n",
    "                tree_to_indexes[most_freq_tree].append(i)\n",
    "                i += 1\n",
    "                # journal = preprocessor.preprocess_journal(article['journal'])\n",
    "                # wf.write(f\"{most_freq_tree}\\n\")\n",
    "                # wf.write(f\"{vector};{most_freq_tree};{journal}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random.seed(75)\n",
    "train_set_not_balanced: List[int] = []\n",
    "test_set_not_balanced: List[int] = []\n",
    "for tree, indexes in tree_to_indexes.items():\n",
    "    my_set = random.sample(indexes, int(0.0006*len(indexes)))\n",
    "    test_set_len = int(0.1*len(my_set))\n",
    "    random.shuffle(my_set)\n",
    "    test_set_not_balanced.extend(my_set[:test_set_len])\n",
    "    train_set_not_balanced.extend(my_set[test_set_len:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "train_set_balanced: List[int] = []\n",
    "test_set_balanced: List[int] = []\n",
    "total_len = len(train_set_not_balanced) + len(test_set_not_balanced)\n",
    "one_tree_set_len = int(total_len / len(tree_to_indexes.keys()))\n",
    "for tree, indexes in tree_to_indexes.items():\n",
    "    my_set = random.sample(indexes, min(one_tree_set_len, len(indexes)))\n",
    "    test_set_len = int(0.1*len(my_set))\n",
    "    random.shuffle(my_set)\n",
    "    test_set_balanced.extend(my_set[:test_set_len])\n",
    "    train_set_balanced.extend(my_set[test_set_len:])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_s_nb = set(train_set_not_balanced)\n",
    "test_s_nb = set(test_set_not_balanced)\n",
    "train_s_b = set(train_set_balanced)\n",
    "test_s_b = set(test_set_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15559157/15559157 [00:04<00:00, 3542527.81it/s]\n",
      "100%|██████████| 15559157/15559157 [00:04<00:00, 3704997.77it/s]\n"
     ]
    }
   ],
   "source": [
    "include_list_nb = [1 if _ in train_s_nb else 2 if _ in test_s_nb else 0 for _ in tqdm(range(15559157))]\n",
    "include_list_b = [1 if _ in train_s_b else 2 if _ in test_s_b else 0 for _ in tqdm(range(15559157))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving new compare file: 100%|██████████| 15559157/15559157 [02:07<00:00, 122164.01it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('/Volumes/SanDisk/BioAsq2021/allMeSH_2021.json', encoding='ISO-8859-1') as bioasq_file:\n",
    "    with open('/Volumes/SanDisk/compare_train_title_b.csv', 'w') as wf_train:\n",
    "        with open('/Volumes/SanDisk/compare_test_title_b.csv', 'w') as wf_test:\n",
    "            for article, include_flag in zip(tqdm(ijson.items(bioasq_file, 'articles.item'), total=15559157, desc='Saving new compare file'), include_list_b):\n",
    "                if include_flag == 0:\n",
    "                    continue\n",
    "                nums: List[int] = preprocessor.preprocess_mesh(article['meshMajor'])\n",
    "                trees = list(filter(lambda x: x is not None, [num_to_tree[n] if n in num_to_tree else None for n in nums]))\n",
    "                most_freq_tree = max(set(trees), key=trees.count)\n",
    "                text_vector = [preprocessor.num_to_text[num] for num in preprocessor.preprocess_text(article['title'] or '')]\n",
    "                # vector = ','.join([str(_) for _ in preprocessor.preprocess_text(article['abstractText'])])\n",
    "                write_text = f\"{' '.join(list(map(str, text_vector)))};{str(most_freq_tree)};{str(article['pmid'])}\\n\"\n",
    "                if include_flag == 1:\n",
    "                    wf_train.write(write_text)\n",
    "                else:\n",
    "                    wf_test.write(write_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84019it [00:01, 55795.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# compute tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "with open('/Volumes/SanDisk/compare_train.csv') as f:\n",
    "    vectors: List[List[str]] = []\n",
    "    labels: List[str] = []\n",
    "    journals: List[str] = []\n",
    "    for line in tqdm(f):\n",
    "        vector_str, y, journal = line.split(';')\n",
    "        vector: List[str] = vector_str.split(',')\n",
    "        vectors.append(vector)\n",
    "        labels.append(y)\n",
    "        journals.append(journal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "term_to_documents_count: Dict[str, int] = {}\n",
    "for vector in vectors:\n",
    "    unique_words = set(vector)\n",
    "    for w in unique_words:\n",
    "        if w not in term_to_documents_count:\n",
    "            term_to_documents_count[w] = 0\n",
    "        term_to_documents_count[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "documents_len = len(vectors)\n",
    "term_to_idf: Dict[str, float] = {}\n",
    "for term, belonging_doc_count in term_to_documents_count.items():\n",
    "    term_to_idf[term] = math.log(documents_len/belonging_doc_count, math.e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 19/119359 [00:03<5:18:01,  6.25it/s]/usr/local/anaconda3/envs/indexing/lib/python3.8/site-packages/sklearn/decomposition/_incremental_pca.py:348: RuntimeWarning: Mean of empty slice.\n",
      "  self.noise_variance_ = explained_variance[self.n_components_ :].mean()\n",
      "/usr/local/anaconda3/envs/indexing/lib/python3.8/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "  3%|▎         | 3446/119359 [21:14<11:54:35,  2.70it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-81-4c30e3e4a1b4>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0mi\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mterm\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0midf\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtqdm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mterm_to_idf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 9\u001B[0;31m     \u001B[0mtfidf_list\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mvector\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcount\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mterm\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0midf\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mvector\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mvectors\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     10\u001B[0m     \u001B[0mi\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-81-4c30e3e4a1b4>\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0mi\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mterm\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0midf\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtqdm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mterm_to_idf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 9\u001B[0;31m     \u001B[0mtfidf_list\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mvector\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcount\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mterm\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0midf\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mvector\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mvectors\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     10\u001B[0m     \u001B[0mi\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "term_to_tfidf: Dict[str, List[float]] = {}\n",
    "tfidf_list = []\n",
    "pca = IncrementalPCA(20)\n",
    "i = 0\n",
    "for term, idf in tqdm(term_to_idf.items()):\n",
    "    tfidf_list.append([vector.count(term) * idf for vector in vectors])\n",
    "    i += 1\n",
    "\n",
    "    if i == 20:\n",
    "        pca.partial_fit(np.array(pd.DataFrame(tfidf_list)))\n",
    "        tfidf_list.clear()\n",
    "        i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84019/84019 [00:34<00:00, 2402.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# # process few last\n",
    "# if len(tfidf_list) > 0:\n",
    "#     pca.partial_fit(np.array(pd.DataFrame(tfidf_list)))\n",
    "#\n",
    "# # save pca with pickle\n",
    "# import pickle\n",
    "# with open('/Volumes/SanDisk/pca.pkl', 'w') as f:\n",
    "#     pickle.dump(pca, f)\n",
    "\n",
    "# export train and test vectors\n",
    "with open('/Volumes/SanDisk/compare_train.csv') as f:\n",
    "    with open('/Volumes/SanDisk/compare_train_tfidf.csv', 'w') as wf:\n",
    "        for line in tqdm(f, total=84019):\n",
    "            vector_str, y, journal = line.rstrip().split(';')\n",
    "            vector: List[str] = vector_str.split(',')\n",
    "\n",
    "            tfidf_vector = [vector.count(w) * term_to_idf[w] for w in vector]\n",
    "            wf.write(f\"{vector_str};{y};{journal};{','.join([str(_) for _ in tfidf_vector])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9327it [00:03, 2523.92it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('/Volumes/SanDisk/compare_test.csv') as f:\n",
    "    with open('/Volumes/SanDisk/compare_test_tfidf.csv', 'w') as wf:\n",
    "        for line in tqdm(f):\n",
    "            vector_str, y, journal = line.rstrip().split(';')\n",
    "            vector: List[str] = vector_str.split(',')\n",
    "\n",
    "            tfidf_vector = [vector.count(w) * term_to_idf[w] if w in term_to_idf else None for w in vector]\n",
    "            tfidf_vector = list(filter(lambda w: w is not None, tfidf_vector))\n",
    "            wf.write(f\"{vector_str};{y};{journal};{','.join([str(_) for _ in tfidf_vector])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 8385/9000 [00:00<00:00, 186997.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 8385/9000 [00:00<00:00, 203476.23it/s]\n",
      " 93%|█████████▎| 8385/9000 [00:00<00:00, 195240.40it/s]\n",
      " 93%|█████████▎| 8385/9000 [00:00<00:00, 201296.06it/s]\n",
      " 93%|█████████▎| 8385/9000 [00:00<00:00, 255228.70it/s]\n",
      " 93%|█████████▎| 8385/9000 [00:00<00:00, 210763.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build train abstracts out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 8385/84019 [00:02<00:22, 3399.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build test abstracts out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 930/9327 [00:00<00:02, 2871.10it/s]\n"
     ]
    }
   ],
   "source": [
    "class AbstractIterator:\n",
    "    def __iter__(self):\n",
    "        with open('/Volumes/SanDisk/compare_train_title_b.csv') as f:\n",
    "            for line in tqdm(f, total=9000):\n",
    "                vector_str, y, pmid = line.rstrip().split(';')\n",
    "                vector: List[str] = vector_str.split(' ')\n",
    "                yield gensim.models.doc2vec.TaggedDocument(vector, [i])\n",
    "\n",
    "model = gensim.models.Doc2Vec(vector_size=20, window=10, epochs=5, dbow_words=0)\n",
    "sentences_iterator = AbstractIterator()\n",
    "print(\"Build vocab\")\n",
    "model.build_vocab(sentences_iterator)\n",
    "print(\"Train\")\n",
    "model.train(sentences_iterator, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "print(\"Build train abstracts out\")\n",
    "with open('/Volumes/SanDisk/compare_train_title_b.csv') as f:\n",
    "    with open('/Volumes/SanDisk/compare_train_title_b_d2v.csv', 'w') as wf:\n",
    "        for line in tqdm(f, total=84019):\n",
    "            vector_str, y, pmid = line.rstrip().split(';')\n",
    "            vector: List[str] = vector_str.split(' ')\n",
    "            d2v_vector = model.infer_vector(vector)\n",
    "            wf.write(f\"{vector_str};{y};{pmid};{','.join([str(_) for _ in d2v_vector])}\\n\")\n",
    "print(\"Build test abstracts out\")\n",
    "with open('/Volumes/SanDisk/compare_test_title_b.csv') as f:\n",
    "    with open('/Volumes/SanDisk/compare_test_title_b_d2v.csv', 'w') as wf:\n",
    "        for line in tqdm(f, total=9327):\n",
    "            vector_str, y, pmid = line.rstrip().split(';')\n",
    "            vector: List[str] = vector_str.split(' ')\n",
    "            d2v_vector = model.infer_vector(vector)\n",
    "            wf.write(f\"{vector_str};{y};{pmid};{','.join([str(_) for _ in d2v_vector])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# we dont even try Word2Vec\n",
    "class AbstractIterator:\n",
    "    def __iter__(self):\n",
    "        with open('/Volumes/SanDisk/compare_train.csv') as f:\n",
    "            for line in tqdm(f, total=84019):\n",
    "                vector_str, y, journal = line.rstrip().split(';')\n",
    "                vector: List[str] = vector_str.split(',')\n",
    "                yield vector\n",
    "\n",
    "model = gensim.models.Word2Vec(vector_size=3, window=10, epochs=5, sg=1)\n",
    "sentences_iterator = AbstractIterator()\n",
    "print(\"Build vocab\")\n",
    "model.build_vocab(sentences_iterator)\n",
    "print(\"Train\")\n",
    "model.train(sentences_iterator, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "print(\"Build train abstracts out\")\n",
    "with open('/Volumes/SanDisk/compare_train.csv') as f:\n",
    "    with open('/Volumes/SanDisk/compare_train_w2v.csv', 'w') as wf:\n",
    "        for line in tqdm(f, total=84019):\n",
    "            vector_str, y, journal = line.rstrip().split(';')\n",
    "            vector: List[str] = vector_str.split(',')\n",
    "            w2v_vector = [model.wv[word] for word in vector]\n",
    "            wf.write(f\"{vector_str};{y};{journal};{','.join([str(_) for _ in d2v_vector])}\\n\")\n",
    "print(\"Build test abstracts out\")\n",
    "with open('/Volumes/SanDisk/compare_test.csv') as f:\n",
    "    with open('/Volumes/SanDisk/compare_test_d2v.csv', 'w') as wf:\n",
    "        for line in tqdm(f, total=9327):\n",
    "            vector_str, y, journal = line.rstrip().split(';')\n",
    "            vector: List[str] = vector_str.split(',')\n",
    "            d2v_vector = model.infer_vector(vector)\n",
    "            wf.write(f\"{vector_str};{y};{journal};{','.join([str(_) for _ in d2v_vector])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 8385/9000 [00:00<00:00, 154036.88it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "class CorpusIterator:\n",
    "    def __iter__(self):\n",
    "        with open('/Volumes/SanDisk/compare_train_title_b_d2v.csv') as f:\n",
    "            for line in tqdm(f, total=9000):\n",
    "                simple_vec_str, y, pmid, d2v_vector_str = line.rstrip().split(';')\n",
    "                d2v_vector: List[float] = [float(_) for _ in d2v_vector_str.split(',')]\n",
    "                yield d2v_vector, y\n",
    "\n",
    "class TestCorpusIterator:\n",
    "    def __iter__(self):\n",
    "        with open('/Volumes/SanDisk/compare_test_title_b_d2v.csv') as f:\n",
    "            for line in tqdm(f, total=9327):\n",
    "                simple_vec_str, y, pmid, d2v_vector_str = line.rstrip().split(';')\n",
    "                d2v_vector: List[float] = [float(_) for _ in d2v_vector_str.split(',')]\n",
    "                yield d2v_vector, y\n",
    "\n",
    "\n",
    "features = []\n",
    "y = []\n",
    "for vector, label in CorpusIterator():\n",
    "    features.append(vector)\n",
    "    y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 930/9327 [00:00<00:00, 151295.58it/s]\n"
     ]
    }
   ],
   "source": [
    "test_features = []\n",
    "test_y = []\n",
    "for vector, label in TestCorpusIterator():\n",
    "    test_features.append(vector)\n",
    "    test_y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(features)\n",
    "test_X = scaler.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/anaconda3/envs/indexing/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      " 10%|█         | 1/10 [00:07<01:05,  7.25s/it]/usr/local/anaconda3/envs/indexing/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      " 20%|██        | 2/10 [00:18<01:16,  9.53s/it]/usr/local/anaconda3/envs/indexing/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      " 30%|███       | 3/10 [00:28<01:07,  9.59s/it]/usr/local/anaconda3/envs/indexing/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      " 40%|████      | 4/10 [00:34<00:50,  8.43s/it]/usr/local/anaconda3/envs/indexing/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      " 50%|█████     | 5/10 [00:41<00:39,  7.81s/it]/usr/local/anaconda3/envs/indexing/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      " 60%|██████    | 6/10 [00:49<00:31,  7.85s/it]/usr/local/anaconda3/envs/indexing/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      " 70%|███████   | 7/10 [00:56<00:23,  7.73s/it]/usr/local/anaconda3/envs/indexing/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      " 80%|████████  | 8/10 [01:03<00:14,  7.45s/it]/usr/local/anaconda3/envs/indexing/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      " 90%|█████████ | 9/10 [01:11<00:07,  7.49s/it]/usr/local/anaconda3/envs/indexing/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "100%|██████████| 10/10 [01:19<00:00,  7.91s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.09462365591397849"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "model = MLPClassifier(hidden_layer_sizes=(100,), random_state=75)\n",
    "for i in tqdm(range(10)):\n",
    "    model.fit(X, y)\n",
    "accuracy_score(test_y, model.predict(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  2.83it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.07419354838709677"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt_model = DecisionTreeClassifier(random_state=75)\n",
    "for i in tqdm(range(10)):\n",
    "    dt_model.fit(X, y)\n",
    "accuracy_score(test_y, dt_model.predict(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP\n",
      "[[ 4  7  5  6  6  1  2  1  2  5 12  4  6  1  0]\n",
      " [ 2  3  6 10  8  5  2  4  2  4  4  2  2  4  4]\n",
      " [ 4  1  4  8  1  7  7  4  5  2 10  3  0  4  2]\n",
      " [ 7  3  4 13  2  2  6  3  1  4  4  6  1  2  4]\n",
      " [ 5  5  1  5  8  4  3  4  1  4  7  4  5  3  3]\n",
      " [ 1  5  6  7  4  3  2  6  2  7  2  2  7  5  3]\n",
      " [ 5  5  2  3  7  6  8  2  2  2  5  5  3  5  2]\n",
      " [ 3  4  3  2  5  5  2  6  6  5  7  3  3  5  3]\n",
      " [ 1  3  3  3  3  1  6  5  3  6  6  6  2 10  4]\n",
      " [ 1  3  2  7  8  5  6  5  2  4  7  6  2  2  2]\n",
      " [ 1  2  6  4  4  3  1  2  3  4 16  3  3  7  3]\n",
      " [ 0  7  7  5  1  5  4  4  3  3  6  4  3  7  3]\n",
      " [ 2  5  2  4  6  2  2  1  5  4 11  4  2  7  5]\n",
      " [ 6  2  2  6  5  2  5  2  3  6  6  5  4  6  2]\n",
      " [ 4  3  3  6  4  2  2  2  1  2 11  6  7  5  4]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.09      0.06      0.07        62\n",
      "           B       0.05      0.05      0.05        62\n",
      "           C       0.07      0.06      0.07        62\n",
      "           D       0.15      0.21      0.17        62\n",
      "           E       0.11      0.13      0.12        62\n",
      "           F       0.06      0.05      0.05        62\n",
      "           G       0.14      0.13      0.13        62\n",
      "           H       0.12      0.10      0.11        62\n",
      "           I       0.07      0.05      0.06        62\n",
      "           J       0.06      0.06      0.06        62\n",
      "           K       0.14      0.26      0.18        62\n",
      "           L       0.06      0.06      0.06        62\n",
      "           M       0.04      0.03      0.04        62\n",
      "           N       0.08      0.10      0.09        62\n",
      "           Z       0.09      0.06      0.08        62\n",
      "\n",
      "    accuracy                           0.09       930\n",
      "   macro avg       0.09      0.09      0.09       930\n",
      "weighted avg       0.09      0.09      0.09       930\n",
      "\n",
      "Decision tree\n",
      "[[ 2  5  6  6  4  1  5  4  5  1  9  2  6  3  3]\n",
      " [ 4  4  7  5  3  2  6  3  6  3  8  5  2  4  0]\n",
      " [ 2  5  5  2  2  1  6  1 10  9  5  6  3  4  1]\n",
      " [ 5  4  3  3  2  3  7  3  1  6  3  5  6  6  5]\n",
      " [ 6  5  4 10  6  5  3  5  2  4  0  3  3  3  3]\n",
      " [ 2  4  6  6  6  3  5  3  3  4  4  3  3  4  6]\n",
      " [ 0  2  9  4  2  5  6  5  5  3  4  2  5  4  6]\n",
      " [ 4  2  5  1  5  7  2  4  8  1  6  7  3  4  3]\n",
      " [ 4  2  5  5  4  6  7  4  7  3  4  4  3  3  1]\n",
      " [ 5  2  4  3  5  5  2  7  4  4  3  4  2  5  7]\n",
      " [ 5  6  4  1  5  4  4  6  3  2  7  4  4  4  3]\n",
      " [ 8  5  7  4  4  4  3  4  2  3  3  5  4  5  1]\n",
      " [ 6  1  3  3  4  2  3  5  5  6  1  5  6  5  7]\n",
      " [ 2  6  5  3  4  7  3  6  2  3  2  5  8  2  4]\n",
      " [ 4  1  3  3  4  4  2  2  6  5  3  8  6  6  5]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.03      0.03      0.03        62\n",
      "           B       0.07      0.06      0.07        62\n",
      "           C       0.07      0.08      0.07        62\n",
      "           D       0.05      0.05      0.05        62\n",
      "           E       0.10      0.10      0.10        62\n",
      "           F       0.05      0.05      0.05        62\n",
      "           G       0.09      0.10      0.10        62\n",
      "           H       0.06      0.06      0.06        62\n",
      "           I       0.10      0.11      0.11        62\n",
      "           J       0.07      0.06      0.07        62\n",
      "           K       0.11      0.11      0.11        62\n",
      "           L       0.07      0.08      0.08        62\n",
      "           M       0.09      0.10      0.10        62\n",
      "           N       0.03      0.03      0.03        62\n",
      "           Z       0.09      0.08      0.09        62\n",
      "\n",
      "    accuracy                           0.07       930\n",
      "   macro avg       0.07      0.07      0.07       930\n",
      "weighted avg       0.07      0.07      0.07       930\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(\"MLP\")\n",
    "mlp_predict_y = model.predict(test_X)\n",
    "print(confusion_matrix(test_y, mlp_predict_y))\n",
    "print(classification_report(test_y, mlp_predict_y))\n",
    "print(\"Decision tree\")\n",
    "dt_predict_y = dt_model.predict(test_X)\n",
    "print(confusion_matrix(test_y, dt_predict_y))\n",
    "print(classification_report(test_y, dt_predict_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84019/84019 [00:04<00:00, 18697.93it/s]\n",
      "100%|██████████| 9327/9327 [00:00<00:00, 19448.23it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (84019,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-130-78661adc70b9>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[0mscaler\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mStandardScaler\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 34\u001B[0;31m \u001B[0mX\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mscaler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit_transform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     35\u001B[0m \u001B[0mtest_X\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mscaler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtest_features\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmetrics\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0maccuracy_score\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/anaconda3/envs/indexing/lib/python3.8/site-packages/sklearn/base.py\u001B[0m in \u001B[0;36mfit_transform\u001B[0;34m(self, X, y, **fit_params)\u001B[0m\n\u001B[1;32m    850\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0my\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    851\u001B[0m             \u001B[0;31m# fit method of arity 1 (unsupervised transformation)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 852\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mfit_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    853\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    854\u001B[0m             \u001B[0;31m# fit method of arity 2 (supervised transformation)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/anaconda3/envs/indexing/lib/python3.8/site-packages/sklearn/preprocessing/_data.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[1;32m    804\u001B[0m         \u001B[0;31m# Reset internal state before fitting\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    805\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_reset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 806\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpartial_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msample_weight\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    807\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    808\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mpartial_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msample_weight\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/anaconda3/envs/indexing/lib/python3.8/site-packages/sklearn/preprocessing/_data.py\u001B[0m in \u001B[0;36mpartial_fit\u001B[0;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[1;32m    839\u001B[0m         \"\"\"\n\u001B[1;32m    840\u001B[0m         \u001B[0mfirst_call\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"n_samples_seen_\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 841\u001B[0;31m         X = self._validate_data(\n\u001B[0m\u001B[1;32m    842\u001B[0m             \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    843\u001B[0m             \u001B[0maccept_sparse\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"csr\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"csc\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/anaconda3/envs/indexing/lib/python3.8/site-packages/sklearn/base.py\u001B[0m in \u001B[0;36m_validate_data\u001B[0;34m(self, X, y, reset, validate_separately, **check_params)\u001B[0m\n\u001B[1;32m    564\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Validation should be done on X, y or both.\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    565\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mno_val_X\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mno_val_y\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 566\u001B[0;31m             \u001B[0mX\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcheck_array\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mcheck_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    567\u001B[0m             \u001B[0mout\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    568\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0mno_val_X\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mno_val_y\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/anaconda3/envs/indexing/lib/python3.8/site-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36mcheck_array\u001B[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001B[0m\n\u001B[1;32m    744\u001B[0m                     \u001B[0marray\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0marray\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcasting\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"unsafe\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcopy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    745\u001B[0m                 \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 746\u001B[0;31m                     \u001B[0marray\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0masarray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0morder\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0morder\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    747\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mComplexWarning\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mcomplex_warning\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    748\u001B[0m                 raise ValueError(\n",
      "\u001B[0;32m/usr/local/anaconda3/envs/indexing/lib/python3.8/site-packages/numpy/core/_asarray.py\u001B[0m in \u001B[0;36masarray\u001B[0;34m(a, dtype, order, like)\u001B[0m\n\u001B[1;32m    100\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0m_asarray_with_like\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0morder\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0morder\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlike\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlike\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    101\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 102\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcopy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0morder\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0morder\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    103\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    104\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (84019,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "class CorpusIterator:\n",
    "    def __iter__(self):\n",
    "        with open('/Volumes/SanDisk/compare_train_tfidf.csv') as f:\n",
    "            for line in tqdm(f, total=84019):\n",
    "                simple_vec_str, y, journal, d2v_vector_str = line.rstrip().split(';')\n",
    "                d2v_vector: List[float] = [float(_) for _ in d2v_vector_str.split(',')]\n",
    "                yield d2v_vector, y\n",
    "\n",
    "\n",
    "class TestCorpusIterator:\n",
    "    def __iter__(self):\n",
    "        with open('/Volumes/SanDisk/compare_test_tfidf.csv') as f:\n",
    "            for line in tqdm(f, total=9327):\n",
    "                simple_vec_str, y, journal, d2v_vector_str = line.rstrip().split(';')\n",
    "                d2v_vector: List[float] = [float(_) for _ in d2v_vector_str.split(',')]\n",
    "                yield d2v_vector, y\n",
    "\n",
    "\n",
    "features = []\n",
    "y = []\n",
    "for vector, label in CorpusIterator():\n",
    "    features.append(vector)\n",
    "    y.append(label)\n",
    "test_features = []\n",
    "test_y = []\n",
    "for vector, label in TestCorpusIterator():\n",
    "    test_features.append(vector)\n",
    "    test_y.append(label)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(features)\n",
    "test_X = scaler.transform(test_features)\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model = MLPClassifier(hidden_layer_sizes=(60,), random_state=75)\n",
    "for i in tqdm(range(10)):\n",
    "    model.fit(X, y)\n",
    "accuracy_score(test_y, model.predict(test_X))\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_model = DecisionTreeClassifier(random_state=75)\n",
    "for i in tqdm(range(10)):\n",
    "    dt_model.fit(X, y)\n",
    "accuracy_score(test_y, dt_model.predict(test_X))\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"MLP\")\n",
    "mlp_predict_y = model.predict(test_X)\n",
    "print(confusion_matrix(test_y, mlp_predict_y))\n",
    "print(classification_report(test_y, mlp_predict_y))\n",
    "print(\"Decision tree\")\n",
    "dt_predict_y = dt_model.predict(test_X)\n",
    "print(confusion_matrix(test_y, dt_predict_y))\n",
    "print(classification_report(test_y, dt_predict_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84019"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}